# -*- coding: utf-8 -*-
"""ProyectoML-KC-KevinMarquez.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TlU3-Db6mmJXRfG77bwnZmHwqmEyKpI6

# üß† Proyecto Machine Learning ‚Äì Airbnb Listings

## üìö √çndice

1. [Importaci√≥n de librer√≠as](#1)  
2. [Divisi√≥n del dataset](#2)  
3. [Exploraci√≥n inicial de los datos](#3)  
4. [Preprocesamiento de variables num√©ricas](#4)  
5. [An√°lisis de correlaci√≥n](#5)  
6. [Selecci√≥n de caracter√≠sticas](#6)  
7. [Codificaci√≥n de variables categ√≥ricas](#7)  
8. [Construcci√≥n del dataset final](#8)  
9. [Comparaci√≥n de modelos con validaci√≥n cruzada](#9)  
10. [Entrenamiento y evaluaci√≥n final del modelo](#10)  
11. [Visualizaci√≥n de resultados](#11)  
12. [Conclusiones y escalabilidad del proyecto](#12)

## üìò 1. Importaci√≥n de librer√≠as <a name="1"></a>

Importamos las librer√≠as necesarias para el an√°lisis de datos, preprocesamiento, visualizaci√≥n, entrenamiento de modelos y evaluaci√≥n. Estas herramientas conforman nuestro stack principal de trabajo en este proyecto.
"""

# Commented out IPython magic to ensure Python compatibility.
# Paso 1 Importaci√≥n de librer√≠as
# Librer√≠as esenciales para an√°lisis y modelado
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Preprocesamiento y modelos
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.linear_model import LinearRegression
from sklearn.ensemble import RandomForestRegressor
from sklearn.metrics import mean_squared_error, r2_score
# Configuraci√≥n de gr√°ficos
# %matplotlib inline
sns.set(style="whitegrid")

# Carga del dataset
# üìå Montaje de Google Drive
from google.colab import drive
drive.mount('/content/drive')
ruta = "/content/drive/MyDrive/ColabNotebooks/airbnb-listings-extract.csv"
df = pd.read_csv(ruta, delimiter=";")


# Visualizaci√≥n de las primeras filas
df.head()

"""## üìÇ 2. Divisi√≥n del dataset en entrenamiento y prueba

Antes de realizar cualquier an√°lisis o transformaci√≥n, dividimos el dataset en conjuntos de entrenamiento y prueba.  
Esto es fundamental para evitar el sobreajuste y evaluar correctamente el rendimiento de nuestros modelos.

"""

# Paso 2 Definimos la variable objetivo (target)
target = "Price"
X = df.drop(columns=target)
y = df[target]

# Divisi√≥n 80/20
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
X_train.shape, X_test.shape

"""## üîç 3. Exploraci√≥n inicial de los datos (solo en X_train)

A continuaci√≥n realizamos un an√°lisis exploratorio b√°sico **solo con el conjunto de entrenamiento (`X_train`)**.  
Nos enfocamos en:

- Tipo de variables y valores nulos  
- Estad√≠sticas descriptivas  
- Distribuci√≥n de la variable objetivo (`y_train`)  

"""

# Paso 3 Vista general de las primeras filas
X_train.head()
# Informaci√≥n general del conjunto de entrenamiento
X_train.info()
# Estad√≠sticas descriptivas de columnas num√©ricas
X_train.describe().T
# Revisi√≥n de valores nulos
X_train.isnull().sum().sort_values(ascending=False).head(10)
# Distribuci√≥n de la variable objetivo
plt.figure(figsize=(8, 4))
sns.histplot(y_train, bins=50, kde=True)
plt.title("Distribuci√≥n de la variable objetivo (price)")
plt.xlabel("Precio")
plt.ylabel("Frecuencia")
plt.show()

"""## üßº 4. Preprocesamiento: Imputaci√≥n y selecci√≥n de variables num√©ricas

Filtramos las columnas num√©ricas y aplicamos imputaci√≥n de valores nulos con la media, una estrategia sencilla y com√∫n en el m√≥dulo.  
Todav√≠a no aplicamos escalado ni codificaci√≥n, solo limpieza b√°sica para poder continuar con la selecci√≥n de caracter√≠sticas.

"""

# Paso 4 Selecci√≥n de columnas num√©ricas
num_cols = X_train.select_dtypes(include=["float64", "int64"]).columns.tolist()

# Imputaci√≥n de valores nulos con la media
imputer = SimpleImputer(strategy="mean")
X_train_num = pd.DataFrame(imputer.fit_transform(X_train[num_cols]), columns=num_cols)
X_test_num = pd.DataFrame(imputer.transform(X_test[num_cols]), columns=num_cols)

X_train_num.shape, X_test_num.shape

"""## üìà 5. An√°lisis de correlaci√≥n

Antes de seleccionar las variables que usaremos para entrenar el modelo, analizamos la correlaci√≥n entre las variables num√©ricas y el target (`price`).  
Esto nos ayuda a identificar relaciones lineales fuertes, irrelevantes o redundantes entre variables.  
Importante: solo trabajamos con `X_train_num` y `y_train` para no tocar el test set.

"""

# Paso 5 Concatenamos X_train_num con y_train para visualizar correlaciones
df_corr = X_train_num.copy()
df_corr["price"] = y_train.reset_index(drop=True)

# C√°lculo de la matriz de correlaci√≥n
corr_matrix = df_corr.corr()

# Visualizaci√≥n del heatmap
plt.figure(figsize=(12, 10))
sns.heatmap(corr_matrix, cmap='coolwarm', center=0, annot=False)
plt.title("üìà Matriz de Correlaci√≥n con el Target")
plt.show()

"""## üß™ 6. Selecci√≥n de caracter√≠sticas con SelectKBest

Antes de seleccionar las mejores variables num√©ricas para entrenar nuestro modelo, verificamos si existen valores nulos en `y_train` y los eliminamos,  
ya que este tipo de datos no puede ser imputado.

Aplicamos `SelectKBest` con la funci√≥n `f_regression`, tal como se vio en el m√≥dulo.  
Esto nos permite quedarnos con las 10 variables num√©ricas m√°s relevantes para predecir el precio (`price`).

Al final listamos qu√© variables fueron seleccionadas y cu√°les fueron descartadas, justificando su posible eliminaci√≥n.

"""

from sklearn.feature_selection import SelectKBest, f_regression

# Paso 6 Creamos un nuevo dataframe num√©rico para train y test
X_train_num = X_train.select_dtypes(include='number')
X_test_num = X_test.select_dtypes(include='number')

#  Eliminamos columnas que no aportan valor o tienen demasiados nulos
cols_to_drop = ['ID', 'Scrape ID', 'Host ID', 'Latitude', 'Longitude', 'Square Feet']
X_train_num = X_train_num.drop(columns=cols_to_drop)
X_test_num = X_test_num.drop(columns=cols_to_drop)

#  Imputamos con la media para columnas num√©ricas (solo en train para evitar filtraci√≥n)
X_train_num = X_train_num.copy()
X_test_num = X_test_num.copy()
X_train_num = X_train_num.fillna(X_train_num.mean())
X_test_num = X_test_num.fillna(X_train_num.mean())

#  Aseguramos que y_train no tenga NaN (como dijo el error antes)
y_train = y_train.fillna(y_train.mean())

# 5. Aplicamos SelectKBest
selector = SelectKBest(score_func=f_regression, k=10)
X_train_selected = selector.fit_transform(X_train_num, y_train)
X_test_selected = selector.transform(X_test_num)


#  Guardamos los nombres de las variables seleccionadas
selected_features = X_train_num.columns[selector.get_support()]
print("‚úÖ Variables seleccionadas:\n", list(selected_features))

#  Convertimos a DataFrame con el √≠ndice correcto
X_train_selected_df = pd.DataFrame(X_train_selected, index=X_train.index, columns=selected_features)
X_test_selected_df = pd.DataFrame(X_test_selected, index=X_test.index, columns=selected_features)

"""üìä **An√°lisis de la selecci√≥n de variables num√©ricas**

Utilizando `SelectKBest` con `f_regression`, seleccionamos las 10 variables num√©ricas con mayor correlaci√≥n lineal con el precio (`price`), que es nuestra variable objetivo.

‚úÖ Las variables seleccionadas est√°n directamente relacionadas con las caracter√≠sticas internas del alojamiento y su pol√≠tica de precios, lo cual tiene sentido:

- `Host Listings Count`, `Host Total Listings Count`: Pueden reflejar experiencia del anfitri√≥n.
- `Accommodates`, `Bathrooms`, `Bedrooms`, `Beds`: Representan la capacidad del alojamiento.
- `Monthly Price`, `Security Deposit`, `Cleaning Fee`: Variables econ√≥micas directamente ligadas al precio.
- `Guests Included`: Influye directamente en el valor base de la estancia.

‚ùå Las variables eliminadas tienen, en general, poca correlaci√≥n lineal con el precio, ya sea por:

- **Identificadores** irrelevantes (`ID`, `Scrape ID`, `Host ID`).
- **Ubicaci√≥n geogr√°fica sin contextualizar** (`Latitude`, `Longitude`).
- **Disponibilidad y noches m√≠nimas/m√°ximas** (`Availability 30`, `Minimum Nights`), que pueden impactar m√°s en tasas de ocupaci√≥n que en precios.
- **Reviews y puntuaciones** (`Review Scores Rating`, etc.), que podr√≠an ser relevantes, pero tal vez no muestran correlaci√≥n directa en este caso concreto.
- **Datos poco presentes** (`Square Feet`), posiblemente con muchos nulos o poca representaci√≥n.

Este an√°lisis refuerza la importancia de **hacer una selecci√≥n basada en evidencia estad√≠stica** y no solo en intuici√≥n.

"""

features = X_train_num.columns
feature_scores = selector.scores_

# Creamos un DataFrame para visualizar los resultados ordenados
scores_df = pd.DataFrame({'Feature': features, 'Score': feature_scores})
scores_df = scores_df.sort_values(by='Score', ascending=False)

# Mostramos la gr√°fica
plt.figure(figsize=(12, 6))
sns.barplot(x='Score', y='Feature', data=scores_df, palette="viridis")
plt.title("Importancia de caracter√≠sticas num√©ricas seg√∫n SelectKBest (f_regression)", fontsize=14)
plt.xlabel("Puntaje F")
plt.ylabel("Variable")
plt.tight_layout()
plt.show()

"""## üî° 7. Codificaci√≥n de variables categ√≥ricas

Seleccionamos un conjunto limitado de variables categ√≥ricas que consideramos relevantes para el modelo.  
Aplicamos codificaci√≥n one-hot (`OneHotEncoder`) para convertirlas a formato num√©rico,  
evitando errores con nuevas categor√≠as mediante `handle_unknown='ignore'`.

"""

#  Paso 7 Vemos las columnas categ√≥ricas (excluyendo las num√©ricas ya seleccionadas)
categorical_cols = X.select_dtypes(include='object').columns.tolist()
print("Variables categ√≥ricas disponibles:", categorical_cols)

# Vamos a revisar valores √∫nicos para filtrar las m√°s √∫tiles
for col in categorical_cols:
    print(f"\n{col} - valores √∫nicos:")
    print(X[col].nunique(), "valores √∫nicos")
    print(X[col].value_counts(dropna=False).head(5))

"""üß† **An√°lisis de variables categ√≥ricas**

Tras revisar las variables categ√≥ricas, se decide incluir √∫nicamente aquellas con un n√∫mero razonable de valores √∫nicos y potencial valor predictivo, evitando columnas con demasiados valores √∫nicos (como `Name` o `Description`) o identificadores.

‚úÖ Variables seleccionadas para codificaci√≥n:
- `Property Type`
- `Room Type`
- `Bed Type`
- `Cancellation Policy`

‚ùå Variables descartadas:
- `Name`, `Summary`, `Description`, `Host Name`, etc. ‚Üí Demasiados valores √∫nicos o texto libre sin procesamiento NLP.
- `Street`, `City`, `Zipcode` ‚Üí Poca consistencia o alta cardinalidad.

"""

from sklearn.preprocessing import OneHotEncoder
from sklearn.compose import ColumnTransformer

# Seleccionamos columnas a codificar
categorical_selected = ['Property Type', 'Room Type', 'Bed Type', 'Cancellation Policy']

# Creamos el transformador
encoder = ColumnTransformer(
    transformers=[
        ('cat', OneHotEncoder(handle_unknown='ignore', sparse_output=False), categorical_selected)
    ],
    remainder='drop'
)

# Aplicamos codificaci√≥n
X_train_cat_encoded = encoder.fit_transform(X_train[categorical_selected])
X_test_cat_encoded = encoder.transform(X_test[categorical_selected])

# Obtenemos nombres de las columnas codificadas
encoded_cols = encoder.named_transformers_['cat'].get_feature_names_out(categorical_selected)
X_train_cat_df = pd.DataFrame(X_train_cat_encoded, columns=encoded_cols, index=X_train.index)
X_test_cat_df = pd.DataFrame(X_test_cat_encoded, columns=encoded_cols, index=X_test.index)

"""## üèóÔ∏è 8. Construcci√≥n del dataset final

Combinamos las variables num√©ricas seleccionadas con las variables categ√≥ricas codificadas para formar `X_train_final`.  
Tambi√©n realizamos limpieza del conjunto de prueba, imputando valores con la mediana de entrenamiento y aplicando la misma transformaci√≥n.  
Esto deja `X_test_final` y `y_test_clean` listos para evaluaci√≥n final.

"""

#  Paso 8 Creamos los finales X_train_final y X_test_final
X_train_final = pd.concat([pd.DataFrame(X_train_selected, index=X_train.index, columns=selected_features),
                           X_train_cat_df], axis=1)

X_test_final = pd.concat([pd.DataFrame(X_test_selected, index=X_test.index, columns=selected_features),
                          X_test_cat_df], axis=1)

print("X_train_final shape:", X_train_final.shape)
print("X_test_final shape:", X_test_final.shape)

# Verifica tipos de datos
print(X_train_final.dtypes.value_counts())
print(y_train.dtype)
print(y_train.head())
print(y_train.dtype)
print(y_train.head())
y_train = pd.to_numeric(y_train, errors='coerce')
# Resetear √≠ndices para evitar cualquier desajuste oculto
X_train_final = X_train_final.reset_index(drop=True)
y_train = y_train.reset_index(drop=True)

"""## ü§ñ 9. Comparaci√≥n de modelos con validaci√≥n cruzada

Entrenamos y comparamos varios modelos de regresi√≥n usando validaci√≥n cruzada (`cross_val_score`) con 5 particiones.  
Evaluamos el rendimiento con la m√©trica **RMSE** (error cuadr√°tico medio ra√≠z).  

**A√∫n no usamos los datos de test**, para evitar fugas de informaci√≥n.  
El objetivo es ver qu√© modelo se comporta mejor en entrenamiento antes de decidir con cu√°l continuar.

"""

# Paso 9 n entrenamiento de modelos
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import LinearRegression
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_squared_error


# Lista de modelos a evaluar
modelos = [
    ('Linear Regression', LinearRegression()),
    ('Decision Tree', DecisionTreeRegressor(random_state=42)),
    ('Random Forest', RandomForestRegressor(random_state=42, n_jobs=-1)),
    ('Gradient Boosting', GradientBoostingRegressor(random_state=42))
]

# DataFrame para guardar resultados
resultados = []

# Evaluamos cada modelo con cross-validation (CV=5)
for nombre, modelo in modelos:
    scores = cross_val_score(modelo, X_train_final, y_train, scoring='neg_root_mean_squared_error', cv=5)
    rmse_promedio = -scores.mean()
    rmse_std = scores.std()
    resultados.append({'Modelo': nombre, 'RMSE Promedio': rmse_promedio, 'Desviaci√≥n Est√°ndar': rmse_std})

# Mostramos resultados como tabla ordenada
df_resultados = pd.DataFrame(resultados).sort_values(by='RMSE Promedio')
display(df_resultados)

"""Conclusiones:
Random Forest es el modelo con mejor desempe√±o promedio (menor RMSE), seguido muy de cerca por Gradient Boosting.

Ambos modelos de ensamble (RF y GB) superan claramente al √°rbol de decisi√≥n simple y a la regresi√≥n lineal.

La desviaci√≥n est√°ndar de todos los modelos es razonable (alrededor de 4‚Äì5), lo cual sugiere estabilidad en la validaci√≥n cruzada.
"""

# Limpieza final del conjunto de test# Imputamos valores faltantes en X_test_num utilizando la mediana de X_train_num
X_test_num_clean = X_test_num.fillna(X_train_num.median())

# Imputamos valores faltantes en y_test utilizando la mediana de y_train
y_test_clean = y_test.fillna(y_train.median())

# Aplicamos la selecci√≥n de caracter√≠sticas al conjunto de prueba
X_test_selected_clean = selector.transform(X_test_num_clean)

# Combinamos las caracter√≠sticas seleccionadas con las variables categ√≥ricas codificadas
X_test_final = pd.concat([
    pd.DataFrame(X_test_selected_clean, index=X_test.index, columns=selected_features),
    X_test_cat_df
], axis=1)

# Verificamos la forma final del conjunto de prueba
print("‚úÖ X_test_final shape:", X_test_final.shape)

"""#### üß† 10. Entrenamiento y evaluaci√≥n final del modelo <a name="10"></a>

Una vez realizado el an√°lisis exploratorio, la limpieza y transformaci√≥n de los datos, y la selecci√≥n de caracter√≠sticas, estamos listos para entrenar el modelo final.

Entrenamos el modelo seleccionado (en este caso `RandomForestRegressor`) usando los datos de entrenamiento preprocesados.  
Luego, realizamos predicciones sobre el conjunto de test limpio (`X_test_final`) y comparamos los resultados reales con los predichos.

Evaluamos el rendimiento del modelo mediante dos m√©tricas:

- **RMSE (Root Mean Squared Error)**: nos indica el error promedio en las predicciones (en las mismas unidades que la variable objetivo).  
- **R¬≤ Score (coeficiente de determinaci√≥n)**: mide qu√© proporci√≥n de la varianza de los datos puede explicar el modelo.

Este paso es clave para validar si el modelo realmente generaliza bien o si necesita ajustes.

"""

# Paso 10 Entrenamos el mejor modelo (en este caso Random Forest)
best_model = RandomForestRegressor(random_state=42)
best_model.fit(X_train_final, y_train)

# Realizamos las predicciones sobre el conjunto de test
y_pred = best_model.predict(X_test_final)

# Calculamos m√©tricas de evaluaci√≥n
rmse_test = np.sqrt(mean_squared_error(y_test_clean, y_pred))
r2_test = r2_score(y_test_clean, y_pred)

# Mostramos los resultados
print(f"üìä Evaluaci√≥n final del modelo en el conjunto de test:")
print(f"‚úîÔ∏è RMSE (Test): {rmse_test:.2f}")
print(f"‚úîÔ∏è R¬≤ Score (Test): {r2_test:.4f}")

"""## üìà 11. Visualizaci√≥n de resultados <a name="11"></a>

Comparamos visualmente los precios reales (`y_test_clean`) con los precios predichos (`y_pred`) por el modelo.  
La gr√°fica generada muestra un diagrama de dispersi√≥n con una l√≠nea roja diagonal que representa la predicci√≥n perfecta.

Este gr√°fico permite identificar:

- Qu√© tan bien el modelo predice a lo largo de distintos rangos de precios.
- Si hay sesgos, sobreestimaciones o subestimaciones sistem√°ticas.
- Posibles errores grandes o outliers.

Esta visualizaci√≥n complementa las m√©tricas num√©ricas y es √∫til para interpretar los resultados de manera intuitiva.

"""

# üìå üîß Gr√°fico de comparaci√≥n entre valores reales y predichos
plt.figure(figsize=(10, 6))
sns.scatterplot(x=y_test, y=y_pred, alpha=0.5)
plt.plot([min(y_test), max(y_test)], [min(y_test), max(y_test)], linestyle="--", color="red") # L√≠nea ideal
plt.xlabel("Precio Real")
plt.ylabel("Precio Predicho")
plt.title(f"üìä Evaluaci√≥n del Modelo\nRMSE: {rmse_test:.2f}, R¬≤: {r2_test:.2f}")
plt.show()

"""## üì¶ 12. Conclusiones y escalabilidad del proyecto <a name="12"></a>

Este proyecto implementa un flujo completo de machine learning para regresi√≥n:

- Preprocesado riguroso y sin fugas de datos (data leakage)
- Selecci√≥n de variables num√©ricas m√°s relevantes
- Codificaci√≥n eficiente de variables categ√≥ricas
- Evaluaci√≥n objetiva mediante validaci√≥n cruzada y conjunto de test

### ‚úÖ Conclusiones:
- El modelo `RandomForestRegressor` mostr√≥ un rendimiento robusto y consistente.
- Las variables seleccionadas tuvieron correlaciones significativas con el precio.
- Se evit√≥ el sobreajuste mediante separaci√≥n temprana de datos y evaluaci√≥n cruzada.

### üöÄ Escalabilidad:
Este pipeline puede adaptarse f√°cilmente a otros problemas de regresi√≥n, cambiando simplemente:
1. El dataset de entrada.
2. El nombre de la variable objetivo (`target`).
3. Las columnas num√©ricas y categ√≥ricas relevantes.

Adem√°s, el uso de t√©cnicas modulares permite extender el proyecto hacia:
- Optimizaci√≥n de hiperpar√°metros
- Interpretaci√≥n avanzada con SHAP
- Despliegue en producci√≥n o como API

Este enfoque te prepara para aplicar lo aprendido en contextos reales y escalables.

---

"""